{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install scikit_learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnk4RCG11CXw"
      },
      "outputs": [],
      "source": [
        "# @title Import dependancies\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "R9iJViVZ16u3",
        "outputId": "a9c6ba88-0847-4242-c126-fad23960f787"
      },
      "outputs": [],
      "source": [
        "# @title Create dataset with 100 datas and 2 variable\n",
        "x,y = make_blobs(n_samples=100, n_features=2, centers=2, random_state=0)\n",
        "y = y.reshape((y.shape[0], 1))\n",
        "\n",
        "# displays x and y dimensions\n",
        "print(\"dimension of x:\", x.shape)\n",
        "print(\"dimension of y:\", y.shape)\n",
        "\n",
        "plt.scatter(x[:,0], x[:,1], c=y, cmap=\"summer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbQmFLHn5CBq",
        "outputId": "a42399af-1ffe-4779-9cdd-a5f382fb2809"
      },
      "outputs": [],
      "source": [
        "# @title createt initialization function\n",
        "\n",
        "def initialization(X):\n",
        "  \"\"\"\n",
        "  ceci nous donnera un vecteur W (2,1 ) car l'idée c'est d'avoir un vecteur\n",
        "  w qui contient autant de parametre qu'il y'a de variable\n",
        "  \"\"\"\n",
        "  W = np.random.randn(X.shape[1], 1)\n",
        "\n",
        "  \"\"\" pour le parametre b(biais) nous lui passons un nombre réel\n",
        "  car la fonction d'initialisation est z = w1x1 + w2x2 + b\n",
        "  \"\"\"\n",
        "  b = np.random.randn(1)\n",
        "\n",
        "  return (W, b)\n",
        "\n",
        "# test\n",
        "W, b = initialization(x)\n",
        "print(W.shape)\n",
        "print(b.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYXqIDmJ5Q6O",
        "outputId": "a02f5d27-8e2f-4228-9fab-d92bc25d9c42"
      },
      "outputs": [],
      "source": [
        "# @title implement our model function\n",
        "def model(X, W, b):\n",
        "  \"\"\"\n",
        "    the first things we are doing is build Z function (Z= XW + b)\n",
        "    then we are compute activation function A = 1 / 1 + e(-Z)\n",
        "  \"\"\"\n",
        "  Z = X.dot(W) + b\n",
        "  A = 1 / (1 + np.exp(-Z))\n",
        "\n",
        "  return A\n",
        "\n",
        "A = model(x, W, b)\n",
        "A.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVzvkcEa5Q3g",
        "outputId": "59386e2c-f9cf-42a5-8334-25b0ac30517d"
      },
      "outputs": [],
      "source": [
        "# @title implement the Log Loss function(fonction coût)\n",
        "\n",
        "def log_loss(A, y):\n",
        "  \"\"\"\n",
        "    in theorie L = -1/m sum(log(ai)*yi + (1-yi)*log(1-ai))\n",
        "    m = number of data in our dataset then m = len(y)\n",
        "  \"\"\"\n",
        "  epsilon = 1e-15\n",
        "  return  1 / len(y) * np.sum(-y * np.log(A + epsilon) - (1 - y)*np.log(1 - A + epsilon))\n",
        "\n",
        "  # this function return a real number which measure of error our model\n",
        "# test\n",
        "log_loss(A, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUEsO9XJ5Qyf",
        "outputId": "2d759e39-b592-4472-b6fe-bb23dd27e688"
      },
      "outputs": [],
      "source": [
        "# @title Create Gradient function\n",
        "\n",
        "def gradients(A, x, y):\n",
        "  \"\"\"\n",
        "    we have two gradients the jacobien that we note dW and db( derivative of\n",
        "    log_loss function with respect to b)\n",
        "    dW = 1/m * trans(X).(A-Y)\n",
        "    db = 1 /m * sum(A-Y)\n",
        "  \"\"\"\n",
        "  dW = 1 / len(y) * np.dot(x.T, A-y)\n",
        "  db = 1 / len(y) * np.sum(A - y)\n",
        "  return (dW, db)\n",
        "# test\n",
        "dw,db= gradients(A, x, y)\n",
        "print(dw.shape)\n",
        "db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azrDPhZv5QvZ",
        "outputId": "1670a1df-f22f-4ca6-e3f1-5bed112b2fe6"
      },
      "outputs": [],
      "source": [
        "# @title build the update function\n",
        "\n",
        "\"\"\"\n",
        "this function take as input the gradients, W, b and learning rate\n",
        "\"\"\"\n",
        "def update(dW, db, W, b, learning_rate):\n",
        "  # nous allons implementer l'agorithme de la descencte de gradient\n",
        "  \"\"\"\n",
        "  wi = wi - a(dl/dwi)  a= learning rate and (dl/dwi) = dW\n",
        "  bi = bi - a(dl/dbi)  (dl/dbi) = db\n",
        "  \"\"\"\n",
        "  W = W - learning_rate * dW\n",
        "  b = b - learning_rate * db\n",
        "\n",
        "  return (W, b)\n",
        "\n",
        "# test\n",
        "W, b = update(dw, db, W, b, 2)\n",
        "W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl5LXkIr2jX9"
      },
      "outputs": [],
      "source": [
        "# @title create a prediction function\n",
        "\n",
        "def predict(X, W, b):\n",
        "  # computer the output of the model (activation)\n",
        "  A = model(X, W, b)\n",
        "  return A >= 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTu8-C1H6N39"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9_-fCB85QrR"
      },
      "outputs": [],
      "source": [
        "# @title build our Artificial neural\n",
        "\n",
        "\"\"\"\n",
        "  cette fonction va prendre en entré nos données x et y , un pas d'apprentissage\n",
        "  pour notre fonction de mise a jour et nombre d'iteration pour notre algo\n",
        "  d'apprentissage\n",
        "\"\"\"\n",
        "def artificial_neuron(X, y, learning_rate=0.5, n_iter=100):\n",
        "  # initialization of parameter w and b\n",
        "  W,b = initialization(X)\n",
        "\n",
        "  Loss = []\n",
        "  # create learning loop\n",
        "  for i in range(n_iter):\n",
        "    # launch result of our model\n",
        "    A = model(X, W, b)\n",
        "\n",
        "    # capture error of our model\n",
        "    Loss.append(log_loss(A, y))\n",
        "\n",
        "    #create dW, db gradient\n",
        "    dW,db = gradients(A, X, y)\n",
        "\n",
        "    # update W and b parameters\n",
        "    W, b = update(dW,db, W, b, learning_rate)\n",
        "    \n",
        "      \n",
        "  # compute the preddiction of all data x in dataset\n",
        "  # en d'autre terme on calcul ce que la machine predit pour ces san valeurs\n",
        "  y_pred = predict(X,W, b)\n",
        "\n",
        "  # display the performance of our model in computing the accuracy metrics\n",
        "  # print(accuracy_score(y, y_pred))\n",
        "\n",
        "  plt.plot(Loss, c='r')\n",
        "  plt.show()\n",
        "\n",
        "  # then return W and b parameter which model learned\n",
        "  return (W, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pcXe_j_v5Qff",
        "outputId": "a9301a7e-c8b9-42dc-ff2a-da1b311fea02"
      },
      "outputs": [],
      "source": [
        "W, b = artificial_neuron(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "ZfUikcCnvFp5",
        "outputId": "f02ab190-cf90-41e6-ebd6-42f3c1d8aee9"
      },
      "outputs": [],
      "source": [
        "# @title Predict the class of the new_data not in our dataset\n",
        "new_data = np.array([2, 1])\n",
        "\n",
        "#  dessinons la frontiere de décision\n",
        "\"\"\"\n",
        "on sait A = 50% signifie qu'il existe un couple (x1, x2) pour lesquels z = 0\n",
        "z(x1,x2) = 0\n",
        "w1x1 + w2x2 + b = 0\n",
        "pour construire cette droite on a:\n",
        "X1  | X2\n",
        "-2|\n",
        " 08 |\n",
        "pour trouver les valeurs de x2 on se sert de l'équation précedente\n",
        "x2 = (-w1x1 - b) / w2\n",
        "\"\"\"\n",
        "x0 = np.linspace(-2, 4, 100)\n",
        "\n",
        "# print(x0.shape, W[1])\n",
        "x1 = (-W[0]*x0  -b) / W[1]\n",
        "\n",
        "plt.scatter(x[:,0], x[:,1], c=y, cmap=\"summer\")\n",
        "plt.scatter(new_data[0], new_data[1], c=\"r\")\n",
        "plt.plot(x0, x1, c=\"orange\", lw=3)\n",
        "plt.show()\n",
        "\n",
        "predict(new_data, W, b)\n",
        "\"\"\"\n",
        "on constate que grace a cette frontière de décision\n",
        "la machine a predit que une perfomance du modèle sur les 100 données\n",
        "ce qui veut dire que si la performance est 92% alors 8 données ne s'aurait\n",
        "être du bon côté de la frontière de décision\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkVrnNu5vGUm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
